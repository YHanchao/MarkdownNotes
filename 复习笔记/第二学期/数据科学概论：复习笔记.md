---
title: 数据科学概论：复习笔记
date: 2021-05-21 23:54:04
tags:
  - 统计
  - 期末
  - 复习
categories: 学习笔记
mathjax: true
---
最开始是不准备做这个的，无奈这门课实在是知识结构杂乱，而且考试还似乎略显恶心。我在这门课也学得非常糟糕，所以特意总结一次。

<!-- more -->

# 1 数据科学的编程工具

这里主要复习R语言

## 1.1 R的数据类型简介

1. 基础类型：数值、字符、逻辑
2. 向量类型：如何声明、如何运算、怎样引用？

```R
x <- 1: 6
y <- c(2, 4, 6, 8, 10.0)
z <- rep('A', 6)
x * 2
y[c(2, 4)]		# 注：从1开始计数
y[c(FALSE, TRUE, FALSE, TRUE, FALSE)]	# 大写，和Python区分
y[y > 2]
```

3. 矩阵类型：本质还是向量

```R
A1 <- matrix(1: 12, nrow = 3, ncol = 4)
A2 <- matrix(y, nrow = 2)   # 第一个位置是一个向量即可
A1 %*% t(A1) # 区分哈达玛乘积与矩阵成绩
A1[c(2, 3), 1: 2]
```

4. 数组（array）——实际上就是矩阵的扩展，多维的

```R
a <- array(1: 24, dim = c(2, 3, 4))
```

5. 列表：和Python中的类似，其元素可以是任意数据类型

```R
l1 <- list(x1, x2, a)
l1[[1]]   # 返回x1
l2 <- list(m = x1, n = x2, p = a)
l2$n      # 返回x2
l2[['p']] # 返回a
```

6. 数据框

```R
x1 <- c('Peter', 'John', 'Sth')
x2 <- c(18, 19, 20)
dt <- data.frame(Name = x1, Age = x2)
```

## 1.2 R的基础语法

略，这部分和绝大多数语言是一致的，只需要注意有大括号没有分号就行。另外R中的``repeat``和C语言中的``do-while``类似，举例如下：

```R
i <- 1 repeat{
  print(i)
  i <- i + 1
  if (i >= 10) break
}
```

---

# 2 数据科学的数学基础

## 2.1 概率论与数理统计

### 2.1.1 随机变量与分布

随机试验所具有以下三条性质

1. 可以在相同条件下重复进行
2. 每次试验的可能结果不止一个，事先能够明确试验的所有可能结果
3. 每次试验之前不能确定会出现哪一个结果

例：n重伯努利试验：$P(B_k)=C_n^kp^k(1-p)^{n-k}$

简要说说什么是**随机变量**？什么又是**分布**？

简单地说，随机变量指的是，设样本空间是一个集合$\mathbb{S}=\{e\}$，那么实函数$X=X(e)$就是一个随机变量。比如说就以上面的伯努利试验，我们关心的是成功的次数是几次，而不关心究竟是哪几次成功了，这样的话$X$的值域就是集合$\{0,1,2,\ldots,n\}$，函数的形式实际上就是一个映射。

而**分布**指的是函数$F(x)=P(X\leqslant x)$，其中$x\in(-\infty,+\infty)$。这个函数就叫做$X$的分布函数，简称“分布”。这里稍微注意一下，我们在这里提到的“分布”并没有说$X$是连续的还是离散的，也就是说这个定义是一个更加**底层**的定义

接下来看一看这个定义在随机变量是“离散”和“连续”的情况下的应用

#### 2.1.1.1 离散型随机变量

离散型随机变量将随机变量的取值限定为**有限的**或者**可列的**（注意：可列就可以！不要把它和连续混淆！）

既然是离散的，所以可以定义出来$P(X=x_k)$这种东西，这样一个“离散”的东西，我们就把它叫做**随机变量的分布律**——其实就和高中数学里面做统计题的时候画的那个表格（叫啥忘了）一个意思！只不过我们给了一个从头到尾的一套定义。

小联想：如果把分布律看作一个数列，那么$F(x)$就有些像跳跃的函数，它的值域就像是一个级数

#### 2.1.1.2 连续型随机变量

对于一个随机变量$X$，若$\exists f \geqslant 0,D_f = \mathbb{R}$，使得$\forall x$，$F(x)=\int_{-\infty}^x f(t)dt$，则$X$是连续型随机变量。其中$f(x)$称为**概率密度函数**。

性质：$P(a\leqslant X\leqslant b)=\int_a^b f(t)dt$，$\int_{-\infty}^{+\infty } f(t)dt=1$

这里概率密度函数有点像分布律，但是又不是！因为概率值是一个积分，要有$\Delta x$的，不光光是一个$f(x)$

## 2.2 数理统计简介

总体：具有确定分布的随机变量X <——> 分布函数F(X) <—— 如何推断？

**抽样与样本**：从总体中抽取n个个体进行观察或试验的过程叫**抽样**，得到的n个随机变量（它们彼此独立，与X有相同的分布）称为**样本**，这时样本容量为n；观察这n个样本得到的n个观察值叫**样本值**，也叫样本观察值。不过通常抽样的时候假设抽出来的样本和总体有着相同的分布

**统计量**：设抽到的n个样本为$\{X_1,X_2，\ldots,X_n\}$，观察值为$\{x_1,x_2,\ldots,x_n\}$，则称关于这n个观察值的函数$g(x_1,x_2,\ldots,x_n)$为统计量，其值就是统计量的观察值，**统计量也是随机变量**，以期望为例：

离散型随机变量：若$\sum_k |x_k|p_k < \infty$，则称X存在数学期望，记为$E(X)$
$$
E(X) = \sum_{k \geqslant 1 }x_kp_k
$$

连续型随机变量：如果$\int_{-\infty}^{+\infty}|x|f(x)\,\text{d}x<\infty$，则X存在期望：
$$
E(X) = \int_{-\infty}^{+\infty}xf(x)\,\text{d}x
$$

**期望的性质**：线性性

- 常数$k$的期望是其本身$E(k)=k$
- $E(kX)=kE(X)$
- $E(X_1+X_2)=E(X_1)+E(X_2)$

其他常用的统计量：

- 样本均值
- 样本方差
- 顺序统计量：如最大值、最小值、中位数、四分位点

## 2.3 最优化方法

*这部分全是代码实现，也没讲多少原理，我不想看所以不准备写太多了，等啥时候想写再说吧（其实也是觉得考试应该不可能让我手写那么长的调用包吧）*
下面只写几个用到的思想：

1. 非线性规划：有约束问题与无约束问题——迭代与下降算法（*梯度下降*——不过这样的话要么就是能算梯度、要么就是直接能传入梯度进去）、搜索法（一般来说是搜索方向知道了，课本上那个翻词典的例子就说的差不多了）；有约束的就是在无约束的基础上加一个约束条件，就不在全体空间中搜索了

*6月11日补充*：这里的规划还是有一定**函数形式**的，所以可以考虑用数学的解析法来秋季。但是当函数的形式过于复杂的时候，解析法的计算量会过于大，所以人们就考虑用计算机来计算。**迭代**就是先找一个点，再找个搜索的方向，算出来一个新的点，再根据新的点来找搜索的方向，找到下一个点……如果函数值不断减小（或者我们期待的方向，也可能是极大值）则称这样的算法为**下降算法**

2. 线性规划——多边形逼近，不过没展开讲，只是调用了一个包。PS：我觉得考试也不至于考这个，但是说不准考个高中数学难度的线性规划，不过那个没难度就是了。

---

# 3 数据科学的统计原则（非常重要）

**写在前面**：这部分个人认为**非常重要**！

三大统计原则：**可重复**原则、**可计算**原则、**可预测**原则

## 3.1 可重复原则

什么叫“可重复”：统计结果具有稳定性！**数据**或**模型**有一定程度的扰动的时候，分析结果可以保持相对一致——**数据扰动**、**模型扰动**

下面是可重复原则的思维导图

![可重复原则](https://i.loli.net/2021/05/23/h1PzFIHTGD2RQOu.png)

### 3.1.1 数据扰动：抽样变异性

写在前面：产生数据扰动的几种方式：测量误差、数据清洗和整理的扰动、抽样变异性等

**抽样变异性**这个概念是和**推断统计**关系较为紧密的，所以需要事先明确几个关键的名词：总体、样本、抽样、**统计量**、**抽样分布**。

统计量就是由样本信息构造的一个函数，比如说平均数、方差，都是“统计量”
抽样分布则是统计量的分布状况：

- 精确分布：假定$Y\sim N(\mu, \sigma^2)$，则$\bar(Y)\sim N(\mu,\frac{\sigma^2}{n})$，其中n是样本容量
- 渐进分布：由**中心极限定理**导出抽样分布

#### 3.1.1.1 抽样变异性的度量

##### 3.1.1.1.1 统计量的变异性

###### 3.1.1.1.1.1 参数方法：使用抽样分布计算统计量的方差

**特点：** 计算简单，但对总体分布有着较为严格的假定，有时还要再估计$\sigma$

**以最小二乘法为例**，来估计系数的变异性

*（以下内容来源：百度百科）*

设$(x,y)$为观测量，其中$x = (x_1,x_2,\ldots, x_n)^T\in \mathbb{R}^n$，$y\in \mathbb{R}$，且二者满足以下理论函数：$$y=f(x,\omega)$$其中$\omega=(\omega_1,\omega_2,\ldots,\omega_n)^T$是待定参数

那么要找到最优的一组$\omega$的值（何为最优？$f(x,\omega)$与$y$的差别尽量小，尽量拟合），通常会给定$m(m\geqslant n)$组观测数据$(x_i,y_i)\,(i=1,2,\ldots,m)$，来求解目标函数$$L(y,f(x,\omega))=\sum_{i=1}^m[y_i-f(x_i,\omega)]^2$$只需要找到使$L$最小的$\omega$即可

那么再回到参数估计上来。这次我们取
$$f(x,\omega)=
\begin{bmatrix}
  x_{11}&x_{12}&\ldots&x_{1n}\\
  x_{21}&x_{22}&\ldots&x_{2n}\\
  \vdots&\vdots&\ddots&\vdots\\
  x_{m1}&x_{m2}&\ldots&x_{mn}
\end{bmatrix}
\begin{bmatrix}
  \omega_1\\
  \omega_2\\
  \vdots\\
  \omega_n
\end{bmatrix}+
\begin{bmatrix}
  \varepsilon_1\\
  \varepsilon_2\\
  \vdots\\
  \varepsilon_m
\end{bmatrix}
$$

并对模型做出如下假设：

- 正态性：$\varepsilon\sim N(0,\sigma^2)$
- 方差齐性：$\forall x$，$\varepsilon$的方差不变
- 独立性：$\varepsilon$与$x$无关，$\varepsilon$间无关

**这时，参数的变异性体现在所估计的$\omega$上**，课本中给出了$Var(\hat{\omega})=(X'X)^{-1}\sigma^2$，表明$\sigma$的大小、n的取值、X的离散程度均会影响参数估计

*（其实还有其他的例子，比如就拿抽样分布，抽样分布估计出来的均值本身就有样本方差$\frac{\sigma^2}{n}$存在*

（6月11日补充：上面这个流程确实说明了我们要找的那个系数是使$\|X\omega-Y\|$最小的$\omega$，但是这个思想是很“机器学习”的，而不是“最小二乘的”。最小二乘法实际上是一个代数的方法，高等代数课本上把这个解法作为欧氏空间的正交关系的一个使用例有所介绍，具体推导建议参考高等代数）

但是我觉得应该考试不考这个，所以我就不抄了，总之结论如下：

- 欲求的$\omega$是方程：$(X'X)\alpha=X'Y$的解 *（注：我觉得数科概这本书写的有问题，因为不一定$X'X$是满秩的，也就不一定能求逆，但是这个式子在代数上绝对是正确的）*
- $Var(\hat{\omega})=(X'X)^{-1}\sigma^2$，对于参数的误差分析往上几行有，就懒得改了（

###### 3.1.1.1.1.2 非参数方法：以Bootstrap为例

Bootstrap的操作：

1. 从样本$Y=(y_1,y_2,\ldots,y_n)$中进行**n次有放回**的随机抽样，得到了一个Bootstrap样本$Y^{*(1)}$
2. 重复第一步B次，就得到了B个样本
3. 在上述B个Bootstrap样本中，**分别计算**统计量的值，这样就得到了B个统计量的值
4. 估计上述B个统计量的方差

特点：对总体分布的假设宽松，适合难以用抽样分布解决的，但是计算量大

##### 3.1.1.1.2 变量选择的稳定性

为什么要做变量选择？纳入了过多的变量可能导致：模型复杂过拟合、淹没重要的变量、多重共线性……要筛选出那些重要的变量。在正则化方法中，为了解决$m<n$时不能用最小二乘法的问题，提出了一个**稀疏性假设**，就是说，重要的变量相对来说是**少的**，根据这一假设，可以说能够找到足够少的变量来描述我们想要描述的某个统计量——当然这里就又会涉及到统计量的变异性了。

###### 3.1.1.1.2.1 序列不稳定性

基本思路：样本量减小的时候，变量选择是否具有一致性？

做法：

1. 根据**所有样本**选一个**重要变量的集合**
2. 然后随机地从完整样本中删去部分样本，再**用部分样本**重新进行模型拟合；
3. 计算不完全样本所得到的重要变量的集合和基于所有样本得到的重要变量集
合的对称差；
4. 将上述过程重复进行1000 次，求其平均值就可以得到序列不稳定性。

###### 3.1.1.1.2.2 Bootstrap不稳定性

1. 首先用**全部数据**建模，得到$n$个样本的的拟合值$\hat{y_i}$，以及误差项方差的估计值$\hat{\sigma}^2$
2. 记$\bar{y}=\dfrac{1}{n}\sum_{i=1}^n\hat{y_i}$（其实就是全体预测值的平均值），用Bootstrap方法在$N(\bar{y}, \hat{\sigma}^2)$中再抽样（没看懂，这怎么抽）
3. 在Bootstrap样本中再拟合模型，选出重要变量（也没说明白怎么选？似乎这一小步是按照序列不稳定性选）
4. 重复以上过程100次

###### 3.1.1.1.2.3 扰动不稳定性

这个课本上写的真的不清楚，我是看不大懂……所以这次更新也不写了，怕说错。有看懂的同学可以下面发Comment（

#### 3.1.1.2 抽样变异性的解决

##### 3.1.1.2.1 Bagging算法（Bootstrap）

基本思路：抽Bootstrap样本，分别拟合一次，最后平均，用公式写出来就是

$$
\hat{f}(x)=\frac{1}{B}\sum_{b=1}^B\hat{f^{*}}^{(b)}(x)
$$

这样减小了方差，但是改善不了偏差的影响。适用于低偏差高方差的估计量

##### 3.1.1.2.2 随机森林算法

可以用来改善Bootstrap方法中样本所**具有一定的相关度**（也就是并非是完全独立的）的问题

要说明随机森林，首先需要说一说**决策树**的想法。决策树相当于每次输入一个（或者一组）变量，算出来一个什么东西，然后判断是否满足某个条件，以多大概率走向之后的哪一个节点。

随机森林就相当于一大片决策树，所以叫森林（确信）

只不过这里为了降低变量之间的相关性，每次输入树的节点的变量是随机的，之后训练使得找到一个划分最好的变量作为节点分割的依据，这样就能把树延伸下去。

而为了改善Bootstrap方法，可以先拿Bootstrap选出来样本，对于每一个样本都走一遍上面随机森林的过程，最后做个平均（就像Bagging）那样

### 3.1.2 模型扰动

为什么会出现模型的扰动？建模的时候事先假定了总体具有某种分布特征，但是实际上总体可能会**偏离**这种假设的分布特征，从而造成模型扰动。我们期望的是，总体稍微偏离假设的时候，得到的估计结果不会发生太大的变化（稳定性原则）

#### 3.1.2.1 模型扰动的度量

比如说设样本为$x$，和假设的分布$P$（这里$P$是关于*事件*的函数，就像前面对分布的定义那样），那么设添加了扰动之后的分布为$P_{x,t}=(1-t)P+tI(x)$，其中$I$表示扰动

那么再定义这个分布$P$的某个想要研究的特征为$T(P)$（比如说均值$T(P)=\int x\text{d}P(x)$），则扰动的大小就是$$\lim_{t\to 0}\frac{T(P_{x,t})-T(P)}{t}$$也就是说，当有$t$那么多的扰动之后，函数值变化有多快，实际上就是$T(P_{x,t})$的导数

#### 3.1.2.2 稳健估计：M估计

（这段暂时不想看，先搁置）

（6月11日补充：还是不想看，不写了）

## 3.2 可预测原则

感觉这部分相对来说内容轻松一些，这里只是稍微罗列一点内容。**不过，我个人觉得这个地方必考**

**交叉验证法**：将“建立模型”和“预测评价”的数据分开，核心概念：训练集、测试集、验证集、超参数、损失函数；常用的方式：**保留交叉验证、k折交叉验证、留一交叉验证**，具体什么意思作业题留过，各位大作业估计也做过，就不写了，蛮简单的。

稍微注意一下：测试集和验证集的联系和差别。两个都不用来模型训练，只不过测试集用一次来看看模型预测的咋样，是泛化能力；验证机要用好多次看看模型的参数好不好（就是调超参数，毕竟拿不同的数据集测不同的超参数没有意义，所以就要用好多次，这么理解！）

## 3.3 可计算原则

这一部分主要解决的是，在面对超高维、大规模的数据集时如何处理

### 3.3.1 大规模数据的处理方法

分布式存储、并行计算：可以解决计算效率问题，但这类方法得出的结果只有在大规模数据是总体(或对总体有代表性) 时才有意义。甚至可能大规模的样本**代表不了**总体，反而可能夸大了偏倚。分布式存储——如Hadoop；并行计算——多个处理器解决一个问题，时间并行、空间并行

如何抽样能够既保证计算成本可以接受，还能够使得抽到的样本能够反映数据的变异性

这里可能会涉及到：Bootstrap的方法下，假设从$n$个样本的总体中有放回地抽$n$个样本，对于某个样本它没有被抽到的概率是$\lim_{n\to\infty}(1-\dfrac{1}{n})^n=\dfrac{1}{e}$

#### 3.3.1.1 小自助包算法

这个是对Bootstrap的一点改进，总的来说和Bootstrap差不多，只不过是首先先无放回抽好几个小子集，之后再在小子集上使用类似Bootstrap的方法

1. 从$X=\{x_1,x_2,\ldots,x_n\}$中**无放回**随机**抽取**大小为$b$的样本，抽取$s$次（也就是说这一步构造了s个大小为b的子集）
2. 在每个（大小为b的）子样本中，**有放回**地**抽取**大小为$n$的样本，按照蒙特卡洛采样（一种根据概率密度来产生样本的方法）*不是随机*——按照蒙特卡洛法实现了对数据变异性的还原
3. 分别在s个子集中计算统计量
4. 得到全部样本的统计量与不确定性估计

#### 3.3.1.2 子集双重自助法

是对小自助包算法的一点改进，看上去前两步都是一样的，都是先无放回抽几个样本子集，然后每个样本子集拿蒙特卡洛方法有放回地得到蒙特卡洛样本。

只不过之后的步骤变成了在多个子样本中用每个子样本得到的蒙特卡洛样本来刻画估计量的经验分布（我也没看懂）

### 3.3.2 高维、超高维数据的处理方法

高维：变量数大于样本观测数（$p>n$）；超高维：$\log p=O(n^a)\,(a\in(0,1))$

优：信息丰富；劣：训练时间长、成本高；模型复杂、泛化能力差、不易解释

#### 3.3.2.1 高维数据与处理方法

高维数据的一般假设：**稀疏性假设**——众多变量中只有一部分有影响

解决方法：降维（主成分分析、线性成分判别）、变量选择（最优子集选择、正则化方法）

##### 3.3.2.1.1 降维

降维减少变量的方式是：**用原始的变量构造新的变量**

主成分分析的基本思路：构造一个**原始变量的线性组合**，且**最大限度地保留方差信息**——直观来看就是投影的离散程度最大

线性判别的基本思路：在对**类别标签**感兴趣时，希望用**线性方法**降到**一维**，同时还能使得一维空间中还是能够很好判别

##### 3.3.2.1.2 变量选择

最优子集选择：（什么最优？子集包含且只包含重要的变量）通常**穷举**所有子集，燃火按照某一评判标准做评价——一般有三类，信息论准则AIC，贝叶斯方法BIC，预测误差方法Mallow $C_p$

正则化：在损失函数中加入**惩罚项**，实现模型变量的估计、选择【比如说变量的系数训练到0了也就剔除了】（查了一下各种正则化都有，而且根据设定的权重，最后训练效果还不错？在网上能找到L1、L2两种常用的正则化手段）

Lasso正则化（L1）的基本思路实际上是一个带约束的优化问题（就是前面那个带约束优化的意思！）$\min_{\omega}\|y-X\omega\|,\,s.t.\,\|\omega\|\leqslant C$，后面就是说要让$\omega$变小！

#### 3.3.2.2 超高维

基本思路：超高维数据——变量筛选——>高维数据——正则化——数据分析。怎么“筛选”：比较相关性——备选变量与响应变量的相关程度，相关性大就留下。

问题是如何刻画“相关性”？（课件上虽然没写，但是可以大致说一说）

- 皮尔逊（Pearson）相关系数：这个我在另外的课程作业里见过一次，只不过当时这个的表达式还是对余弦相似度的一个修正，这里这个统计量还没学过，估计不考，也不抄了
- 信息熵和信息增益：我觉得这个像是逻辑斯蒂回归里面的损失函数的算法，复习Python课的建议看看（
- 距离计算：什么距离？欧式距离、马氏距离、KL散度……（除了前俩都没听说过）

---

# 4 数据挖掘与机器学习

## 4.1 海量数据、大数据

### 4.1.1 海量数据与数据挖掘

数据挖掘是指从大量数据中通过算法挖掘出其中的信息的过程，最开始是为了解决海量数据的问题

- 常见的数据挖掘技术：分类、回归、异常监测、聚类、关联规则、序列挖掘……
- 主要步骤：
	- 问题理解：明确问题、理解问题（要研究哪些因素？期望结果是什么？当前资源条件是否可行？）
	- 数据理解：理解数据业务意义、探索数据
	- 数据准备：对数据进行整理和转换【比如数据清理 <-- 统计原则】
	- 数据建模：选择模型与损失函数、调参、换模型
	- 模型评估：普适性（模型假设是否可以满足，有偏离会怎样）？有用性（能否预测）？可解释性（是否简洁透明）？新颖性？
  - 模型部署

### 4.1.2 大数据与机器学习

机器学习：任务T、经验E、衡量P

- T：比如回归、聚类
- P：比如回归中的准确率——确保经验可以度量
- E：比如有监督学习、无监督学习——经验来源于数据，从数据中学习，区别于专家系统（基于演绎推理）

## 4.2 无监督学习

无监督学习就是没有标签的、从数据本身提炼信息的，这里讲了主成分分析（研究变量间的关系）和聚类（研究样本间的关系）两种。

主成分分析之前有提到，就不是很想细说了，课本上主要还是给了一个调用库的代码，给了俩图（悬崖碎石图、得分载荷图）。还是注意不要把主成分分析和变量选择这两个搞混了。

### 4.2.1 聚类分析（重要）

- **层次聚类：** 这个作业做了，就不细写了。要点就是类与类之间的距离（有什么距离？欧氏距离、曼哈顿距离$\sum |x_i-y_i|$、明氏距离$[\sum |x_i-y_i|^p]^{\frac{1}{p}}$、余弦距离）
- **K-means聚类：** 这个的思路是：先选定要分成K个类，于是**随机**找K个点作为类中心点，之后分别计算每个点到这K个中心点的距离，离哪个最近就属于哪一类，这样分完一遍以后每个类找重心，作为新的中心点。重复上面的过程（只不过此时类中心点已经找到了就不需要再随机K个中心点）直到类中心点收敛。**缺点：** 这里假设数据分布是簇状的
- **密度聚类：** 首先要明确DBSCAN算法里的三类数据点：核心点、边界点、噪声点
  - 核心点：表明这个点位于类的内部。刻画：这个点的邻域内的点的数量超过一定的阈值
  - 边界点：这个点不是核心点，但是它再核心点的邻域内
  - 噪声点：既不是核心点也不是边界点
  - 这里补充一个网络上的密度聚类的说明，我觉得说的挺清楚：[密度聚类：DBSCAN（博客园）](https://www.cnblogs.com/PJQOOO/p/11838288.html)

## 4.3 有监督学习（重要）

### 4.3.1 回归分析——连续的标签

$$
Y=\beta_0+\beta_1X_1+\beta_2X_2+\ldots+\beta_nX_n+\varepsilon
$$

假设：

- 自变量X非随机变量，因变量Y使随机变量
- **高斯马尔可夫条件：** 随机误差之间不相关，期望为0，方差相等
- 误差项服从正态分布，随机变量间彼此独立
- 样本数大于变量数（$p<n$）

基本上和前面一样这里就不多写了

### 4.3.2 分类问题——不连续的标签

这里主要说**分类性能的评估**。

$$
\begin{matrix}
& \text{预测阳性} & \text{预测阴性}\\
\text{观测阳性} & \text{真阳性TP} & \text{假阴性FN}\\
\text{观测阴性} & \text{假阳性FP} & \text{真阴性TN}
\end{matrix}
$$

一些术语：

- 准确率$=\frac{TP+TN}{TP+FN+FP+TN}$，就是预测对了多少
- 精度$=\frac{TP}{TP+FP}$，就是多少阳性是预测对了——强调我们预测的有多好
- 召回（也是灵敏度）$=\frac{TP}{TP+FN}$，就是预测出来多少阳性——强调我们预测的是否全面，是否不放过
- 特异性$=\frac{TN}{TN+FP}$，就是预测出来多少阴性
- F1：精度和召回的**调和平均数**

这里想补充一下**如何记忆精度和召回**：精度刻画的是在**预测集**中我们的判断如何；**召回**说的是我们在原本的数据集中判断的如何——而且这个名字也比较好联想，毕竟“召回”就是把“原本就是”（TP和FN）的东西预测为“它本来的样子”（TP）

而**特异性**和**灵敏度**这俩是一对儿的，只需要把公式里面的P和N调换以下就行，而灵敏度就是召回

这样就好记一些？！

特异性曲线（ROC曲线）——横坐标：特异性；纵坐标：灵敏度；曲线下面积：AUC

曲线在外侧——相同特异性下灵敏度更高

训练误差、泛化误差——前者是在训练集上得到的误差，后者是测试集上的误差。只在训练集上练习会带来过拟合的问题，如果数据实在不太多，可以用k折交叉验证来做。

#### 4.3.2.1 常用分类模型

1. 逻辑斯蒂回归：略，详见python课
2. 决策树：想法是逐级判断，关键是训练每一个节点如何划分数据。选择最优的划分属性的常用方法：信息增益、增益率、基尼指数等
3. 随机森林：简单地说，随机森林就是一大堆决策树组成的一片森林，只不过这个森林是随机生成的——不过这个方法容易过拟合，所以要多做泛化能力的测试调整
4. 支持向量机：升维分类

---

# 5 数据可视化

这部分主要是代码实现，只是提醒一下其中需要注意的一点点细节。实现的过程就不再写了

好的数据可视化应该有的性质：一是图形需要直观，需要用简单的图反映出需要呈现的信息；二是图形可以展现丰富的信息，要“一图胜千言”；同样要警惕错误的可视化解读！

研究者可以使用统计图形来分析数据、解释数据来找到合适的数据分析方法，也可以通过作图来了解数据的内在规律

## 5.1 可视化中的数据分析

### 5.1.1 单变量的描述分析

主要是明确**什么数据用什么图**，这个原则应当说是普适的！比如说用直方图描述定量数据，用条形图描述定性数据。然后数据分析，不同尺度的数据用的分析方法也不一样，比如说定类的用众数、定序的用中位数四分位数、定量的算均值

### 5.1.2 双变量

双变量之间的关系有：离散-离散；离散-连续；连续-离散；连续-连续

连续-连续：回归分析、相关分析——散点图、折线图……

连续（自变量）-离散（因变量）：分类

离散（自变量）-连续（因变量）：方差分析——箱线图

离散-离散：列联表检验——马赛克图——怎么画？

### 5.1.3 多变量

这个经常出现在拿到数据之后先看看两两变量之间的关系，就涉及到查看相关系数矩阵图的事情。这里课本上给了一个类似热力图的图片

---

# 6 人工智能

写在前面：这里感觉像复习Python……感觉如果有Python课基础应该这里看起来很轻松

# 6.1 神经网络

神经网络的最基本框架：输入层——隐藏层——激活函数——输出函数

感知机模型：没有隐藏层，直接输入层然后加一个线性变换就激活得到输出；

多层感知机（MLP）：有隐藏层

激活函数：比如Sigmoid、ReLU、Softplus（性质比较好的ReLU）、Tanh

前向过程与反向过程：前向过程就是接受输入然后运算输出，就一直往下进行；反向：算梯度（链式法则），反向传播更新参数